services:
  vllm-distributed:
    image: vllm-fixed
    container_name: vllm-distributed
    build:
      context: .
      dockerfile: Dockerfile
    restart: unless-stopped
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    shm_size: 16G
#    devices:
#      - /dev/infiniband:/dev/infiniband
    volumes:
#      - /sys/class/infiniband:/sys/class/infiniband:ro
#      - /dev/infiniband:/dev/infiniband
      - /dev/shm:/dev/shm
      # used by VLLM for compiled model caching, pip cache, etc. Should be unique per container
      - ${ROOT_CACHE_PATH}:/root/.cache
      # the path to where Hugging Face Hub cache is stored. Will use root cache path by default,
      # but should be provided to share a common cache across different containers.
      - ${HUGGINGFACE_HUB_CACHE:-${ROOT_CACHE_PATH}/huggingface}:/root/.cache/huggingface
      - ./src:/distributed
    security_opt:
      - seccomp:unconfined
    cap_add:
      - IPC_LOCK
    environment:
      - HUGGING_FACE_HUB_TOKEN
      - VLLM_HOST_IP=${VLLM_HOST_IP:-127.0.0.1}
      - VLLM_USE_MODELSCOPE
      - VLLM_PP_LAYER_PARTITION
      - NCCL_SOCKET_IFNAME
      - GLOO_SOCKET_IFNAME
      - NCCL_DEBUG=TRACE
    network_mode: host
    ipc: host
    entrypoint: []
    command: python3 /distributed/launch.py ${COMMAND}
    # command: /usr/sbin/sshd -D -p 2222
