services:
  vllm-distributed:
    # image: nvcr.io/nvidia/tritonserver:25.06-vllm-python-py3
    image: vllm-fixed
    build:
      context: .
      dockerfile: Dockerfile.vllm
    restart: unless-stopped
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ${ROOT_CACHE_PATH}:/root/.cache
      - ./src:/distributed
    environment:
      - HUGGING_FACE_HUB_TOKEN
      - VLLM_HOST_IP=${VLLM_HOST_IP:-127.0.0.1}
      - VLLM_SERVER_PORT
      - VLLM_USE_MODELSCOPE
      - NCCL_SOCKET_IFNAME
      - GLOO_SOCKET_IFNAME
    network_mode: host
    ipc: host
    entrypoint: []
    command: python3 /distributed/launch.py ${COMMAND}
    # command: /usr/sbin/sshd -D -p 2222
